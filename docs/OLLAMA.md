# Ollama Setup with Ngrok

## ğŸš€ Download Ollama
Start by downloading the Ollama application from the official website: [Ollama Download](https://ollama.com/). Once installed, Ollama will be running at:
http://localhost:11434

## ğŸ“¦ Pull a Model
Explore the various models available in the Ollama library: [Ollama Library](https://ollama.com/library?sort=popular). To run a model, use the following command:
```
ollama run llama3.1
```

Recommended Models:
- Llama3.1
- Llava (Vision model)

## ğŸŒ Serve Ollama Using Ngrok

### ğŸ”§ Install and Run Ngrok
Visit [Ngrok](https://ngrok.com/docs/getting-started/) Getting Started for installation instructions.
Follow steps 1 to 3 to set up Ngrok on your machine.

### ğŸ“¡ Start Ngrok
Once Ngrok is installed, run the following command to expose your local Ollama server:
```
ngrok http 11434 --host-header="localhost:11434"
```

ğŸ‰ This command will provide you with a public URL to access your Ollama instance.
